{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmF-IEv14txw"
      },
      "outputs": [],
      "source": [
        "# PART 1: SETUP AND DEPENDENCIES\n",
        "# Run this in Google Colab first\n",
        "\n",
        "# Install dependencies\n",
        "!pip install streamlit>=1.28.0\n",
        "!pip install openai-whisper\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install gtts\n",
        "!pip install pydub\n",
        "!pip install groq\n",
        "!pip install python-docx\n",
        "!pip install PyPDF2\n",
        "!pip install soundfile\n",
        "!pip install librosa\n",
        "!pip install torch\n",
        "!pip install pyngrok\n",
        "\n",
        "# System packages for Colab\n",
        "!apt-get update &> /dev/null\n",
        "!apt-get install -y ffmpeg &> /dev/null\n",
        "\n",
        "print(\"✅ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 2: IMPORTS AND CONFIGURATION\n",
        "\n",
        "import streamlit as st\n",
        "import whisper\n",
        "import torch\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import json\n",
        "import os\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import tempfile\n",
        "import io\n",
        "from pathlib import Path\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import logging\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import time\n",
        "from datetime import datetime\n",
        "import base64\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "from groq import Groq\n",
        "import re\n",
        "\n",
        "# For Colab compatibility\n",
        "try:\n",
        "    import IPython.display as ipd\n",
        "    from google.colab import files\n",
        "    COLAB_MODE = True\n",
        "    print(\"✅ Running in Google Colab mode\")\n",
        "except ImportError:\n",
        "    COLAB_MODE = False\n",
        "    print(\"✅ Running in local mode\")\n",
        "\n",
        "# Configuration Class\n",
        "class Config:\n",
        "    \"\"\"Configuration class for the RAG system\"\"\"\n",
        "\n",
        "    # API Configuration\n",
        "    GROQ_API_KEY = \"\"  # Will be set via Streamlit input\n",
        "    GROQ_MODEL = \"llama3-8b-8192\"  # Llama model via Groq\n",
        "\n",
        "    # Model configurations\n",
        "    WHISPER_MODEL = \"base\"  # Options: tiny, base, small, medium, large\n",
        "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"  # Multilingual sentence transformer\n",
        "\n",
        "    # Supported languages\n",
        "    SUPPORTED_LANGUAGES = {\n",
        "        'en': 'English',\n",
        "        'es': 'Spanish',\n",
        "        'fr': 'French',\n",
        "        'de': 'German',\n",
        "        'it': 'Italian',\n",
        "        'pt': 'Portuguese',\n",
        "        'ru': 'Russian',\n",
        "        'zh': 'Chinese',\n",
        "        'ja': 'Japanese',\n",
        "        'ko': 'Korean',\n",
        "        'hi': 'Hindi',\n",
        "        'ar': 'Arabic'\n",
        "    }\n",
        "\n",
        "    # File paths\n",
        "    DOCUMENTS_DIR = \"documents\"\n",
        "    AUDIO_DIR = \"audio\"\n",
        "    EMBEDDINGS_FILE = \"embeddings.index\"\n",
        "    METADATA_FILE = \"metadata.json\"\n",
        "\n",
        "    # Vector database settings\n",
        "    EMBEDDING_DIM = 384\n",
        "    TOP_K_RETRIEVAL = 5\n",
        "\n",
        "    # Audio settings\n",
        "    SAMPLE_RATE = 16000\n",
        "    AUDIO_FORMAT = \"wav\"\n",
        "\n",
        "    # System prompts\n",
        "    SYSTEM_PROMPT = \"\"\"You are an AI assistant designed to help visually impaired users access and understand content.\n",
        "    You provide clear, concise, and helpful responses based on retrieved documents.\n",
        "    Always respond in the same language as the user's question.\n",
        "    Be empathetic and accessibility-focused in your responses.\"\"\"\n",
        "\n",
        "print(\"✅ Configuration loaded successfully!\")"
      ],
      "metadata": {
        "id": "GFgAMya24u2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 3: DOCUMENT PROCESSING\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Handles document loading and preprocessing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.supported_formats = ['.txt', '.pdf', '.docx', '.md']\n",
        "        print(\"✅ DocumentProcessor initialized\")\n",
        "\n",
        "    def load_documents(self, directory: str) -> List[Dict]:\n",
        "        \"\"\"Load and process documents from directory\"\"\"\n",
        "        documents = []\n",
        "\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "            print(f\"Created directory: {directory}\")\n",
        "            return documents\n",
        "\n",
        "        for file_path in Path(directory).rglob('*'):\n",
        "            if file_path.suffix.lower() in self.supported_formats:\n",
        "                try:\n",
        "                    content = self._extract_text(file_path)\n",
        "                    if content.strip():\n",
        "                        chunks = self._chunk_text(content)\n",
        "                        for i, chunk in enumerate(chunks):\n",
        "                            documents.append({\n",
        "                                'id': f\"{file_path.stem}_{i}\",\n",
        "                                'filename': file_path.name,\n",
        "                                'filepath': str(file_path),\n",
        "                                'content': chunk,\n",
        "                                'chunk_index': i,\n",
        "                                'total_chunks': len(chunks)\n",
        "                            })\n",
        "                        print(f\"Processed: {file_path.name} -> {len(chunks)} chunks\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "        print(f\"✅ Loaded {len(documents)} document chunks total\")\n",
        "        return documents\n",
        "\n",
        "    def _extract_text(self, file_path: Path) -> str:\n",
        "        \"\"\"Extract text from different file formats\"\"\"\n",
        "        try:\n",
        "            if file_path.suffix.lower() == '.txt':\n",
        "                return file_path.read_text(encoding='utf-8', errors='ignore')\n",
        "\n",
        "            elif file_path.suffix.lower() == '.pdf':\n",
        "                text = \"\"\n",
        "                with open(file_path, 'rb') as file:\n",
        "                    reader = PyPDF2.PdfReader(file)\n",
        "                    for page in reader.pages:\n",
        "                        text += page.extract_text() + \"\\n\"\n",
        "                return text\n",
        "\n",
        "            elif file_path.suffix.lower() == '.docx':\n",
        "                doc = Document(file_path)\n",
        "                return '\\n'.join([paragraph.text for paragraph in doc.paragraphs])\n",
        "\n",
        "            elif file_path.suffix.lower() == '.md':\n",
        "                return file_path.read_text(encoding='utf-8', errors='ignore')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {file_path}: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "        return \"\"\n",
        "\n",
        "    def _chunk_text(self, text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(words), chunk_size - overlap):\n",
        "            chunk = ' '.join(words[i:i + chunk_size])\n",
        "            if chunk.strip():\n",
        "                chunks.append(chunk)\n",
        "\n",
        "            if i + chunk_size >= len(words):\n",
        "                break\n",
        "\n",
        "        return chunks if chunks else [text]\n",
        "\n",
        "# Test the DocumentProcessor\n",
        "def test_document_processor():\n",
        "    \"\"\"Test the document processor\"\"\"\n",
        "    print(\"Testing DocumentProcessor...\")\n",
        "\n",
        "    # Create test directory and file\n",
        "    test_dir = \"test_documents\"\n",
        "    os.makedirs(test_dir, exist_ok=True)\n",
        "\n",
        "    # Create sample document\n",
        "    with open(f\"{test_dir}/sample.txt\", \"w\") as f:\n",
        "        f.write(\"This is a sample document for testing the RAG system. \" * 20)\n",
        "\n",
        "    # Test processor\n",
        "    processor = DocumentProcessor()\n",
        "    docs = processor.load_documents(test_dir)\n",
        "\n",
        "    print(f\"✅ Test completed: {len(docs)} chunks processed\")\n",
        "    return processor\n",
        "\n",
        "# Run test\n",
        "if __name__ == \"__main__\":\n",
        "    test_processor = test_document_processor()"
      ],
      "metadata": {
        "id": "B2Ax4YV64yYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 4: VOICE PROCESSING\n",
        "\n",
        "class VoiceProcessor:\n",
        "    \"\"\"Handles speech-to-text and text-to-speech conversion\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Loading Whisper model...\")\n",
        "        self.whisper_model = whisper.load_model(Config.WHISPER_MODEL)\n",
        "        self.audio_dir = Config.AUDIO_DIR\n",
        "        os.makedirs(self.audio_dir, exist_ok=True)\n",
        "        print(\"✅ VoiceProcessor initialized\")\n",
        "\n",
        "    def speech_to_text(self, audio_file_path: str) -> Tuple[str, str]:\n",
        "        \"\"\"Convert speech to text using Whisper\"\"\"\n",
        "        try:\n",
        "            print(f\"Transcribing audio: {audio_file_path}\")\n",
        "            result = self.whisper_model.transcribe(audio_file_path)\n",
        "            text = result[\"text\"].strip()\n",
        "            language = result.get(\"language\", \"en\")\n",
        "\n",
        "            print(f\"✅ Transcription completed: '{text}' (language: {language})\")\n",
        "            return text, language\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in speech-to-text: {e}\")\n",
        "            return \"\", \"en\"\n",
        "\n",
        "    def speech_to_text_from_bytes(self, audio_data: bytes) -> Tuple[str, str]:\n",
        "        \"\"\"Convert speech to text from audio bytes\"\"\"\n",
        "        try:\n",
        "            # Save audio data to temporary file\n",
        "            with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_file:\n",
        "                tmp_file.write(audio_data)\n",
        "                tmp_file_path = tmp_file.name\n",
        "\n",
        "            # Transcribe\n",
        "            text, language = self.speech_to_text(tmp_file_path)\n",
        "\n",
        "            # Clean up\n",
        "            os.unlink(tmp_file_path)\n",
        "\n",
        "            return text, language\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in speech-to-text from bytes: {e}\")\n",
        "            return \"\", \"en\"\n",
        "\n",
        "    def text_to_speech(self, text: str, language: str = \"en\") -> bytes:\n",
        "        \"\"\"Convert text to speech using gTTS\"\"\"\n",
        "        try:\n",
        "            print(f\"Generating speech for: '{text[:50]}...' (language: {language})\")\n",
        "\n",
        "            # Map language code for gTTS\n",
        "            tts_lang_map = {\n",
        "                'en': 'en', 'es': 'es', 'fr': 'fr', 'de': 'de', 'it': 'it',\n",
        "                'pt': 'pt', 'ru': 'ru', 'zh': 'zh', 'ja': 'ja', 'ko': 'ko',\n",
        "                'hi': 'hi', 'ar': 'ar'\n",
        "            }\n",
        "            tts_lang = tts_lang_map.get(language, \"en\")\n",
        "\n",
        "            # Generate speech\n",
        "            tts = gTTS(text=text, lang=tts_lang, slow=False)\n",
        "\n",
        "            # Save to bytes\n",
        "            audio_buffer = io.BytesIO()\n",
        "            tts.write_to_fp(audio_buffer)\n",
        "            audio_buffer.seek(0)\n",
        "\n",
        "            print(\"✅ Speech generation completed\")\n",
        "            return audio_buffer.getvalue()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in text-to-speech: {e}\")\n",
        "            # Fallback to English\n",
        "            try:\n",
        "                tts = gTTS(text=text, lang=\"en\", slow=False)\n",
        "                audio_buffer = io.BytesIO()\n",
        "                tts.write_to_fp(audio_buffer)\n",
        "                audio_buffer.seek(0)\n",
        "                return audio_buffer.getvalue()\n",
        "            except:\n",
        "                return b\"\"\n",
        "\n",
        "    def save_audio_file(self, audio_data: bytes, filename: str) -> str:\n",
        "        \"\"\"Save audio data to file\"\"\"\n",
        "        filepath = os.path.join(self.audio_dir, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(audio_data)\n",
        "        print(f\"✅ Audio saved: {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "# Test the VoiceProcessor\n",
        "# Test the VoiceProcessor\n",
        "def test_voice_processor():\n",
        "    \"\"\"Test the voice processor\"\"\"\n",
        "    print(\"Testing VoiceProcessor...\")\n",
        "\n",
        "    # Initialize processor\n",
        "    processor = VoiceProcessor()\n",
        "\n",
        "    # Test text-to-speech\n",
        "    test_text = \"Hello, this is a test of the voice processing system.\"\n",
        "    audio_data = processor.text_to_speech(test_text, \"en\")\n",
        "\n",
        "    if audio_data:\n",
        "        # Save test audio\n",
        "        audio_file = processor.save_audio_file(audio_data, \"test_output.mp3\")\n",
        "        print(f\"✅ Test audio generated: {len(audio_data)} bytes\")\n",
        "\n",
        "        # If in Colab, display audio player\n",
        "        if COLAB_MODE:\n",
        "            ipd.display(ipd.Audio(audio_data))\n",
        "\n",
        "    return processor\n",
        "\n",
        "# Run test\n",
        "if __name__ == \"__main__\":\n",
        "    test_voice = test_voice_processor()\n"
      ],
      "metadata": {
        "id": "E3u-eRlQ41bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 5: VECTOR DATABASE AND RETRIEVAL\n",
        "\n",
        "class VectorDatabase:\n",
        "    \"\"\"Handles document embeddings and similarity search\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedding_model = SentenceTransformer(Config.EMBEDDING_MODEL)\n",
        "        self.index = None\n",
        "        self.documents = []\n",
        "        self.metadata = []\n",
        "        print(\"✅ VectorDatabase initialized\")\n",
        "\n",
        "    def build_index(self, documents: List[Dict]):\n",
        "        \"\"\"Build FAISS index from documents\"\"\"\n",
        "        self.documents = documents\n",
        "\n",
        "        if not documents:\n",
        "            print(\"⚠️ No documents found to index\")\n",
        "            return False\n",
        "\n",
        "        print(f\"Building index for {len(documents)} document chunks...\")\n",
        "\n",
        "        # Extract text content\n",
        "        texts = [doc['content'] for doc in documents]\n",
        "\n",
        "        # Generate embeddings\n",
        "        print(\"Generating embeddings...\")\n",
        "        embeddings = self.embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        # Build FAISS index\n",
        "        self.index = faiss.IndexFlatIP(Config.EMBEDDING_DIM)\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "\n",
        "        # Store metadata\n",
        "        self.metadata = documents\n",
        "\n",
        "        print(f\"✅ Built index with {len(documents)} document chunks\")\n",
        "        return True\n",
        "\n",
        "    def search(self, query: str, k: int = Config.TOP_K_RETRIEVAL) -> List[Dict]:\n",
        "        \"\"\"Search for similar documents\"\"\"\n",
        "        if self.index is None or len(self.documents) == 0:\n",
        "            print(\"⚠️ No index available for search\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Searching for: '{query}'\")\n",
        "\n",
        "        # Generate query embedding\n",
        "        query_embedding = self.embedding_model.encode([query])\n",
        "        query_embedding = query_embedding / np.linalg.norm(query_embedding, axis=1, keepdims=True)\n",
        "\n",
        "        # Search\n",
        "        scores, indices = self.index.search(query_embedding.astype('float32'), k)\n",
        "\n",
        "        # Return results\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx >= 0 and idx < len(self.metadata):  # Valid index check\n",
        "                result = self.metadata[idx].copy()\n",
        "                result['similarity_score'] = float(score)\n",
        "                results.append(result)\n",
        "\n",
        "        print(f\"✅ Found {len(results)} relevant documents\")\n",
        "        for i, result in enumerate(results):\n",
        "            print(f\"  {i+1}. {result['filename']} (score: {result['similarity_score']:.3f})\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_index(self, filepath: str):\n",
        "        \"\"\"Save the FAISS index to file\"\"\"\n",
        "        if self.index is not None:\n",
        "            faiss.write_index(self.index, filepath)\n",
        "\n",
        "            # Save metadata\n",
        "            metadata_path = filepath.replace('.index', '_metadata.json')\n",
        "            with open(metadata_path, 'w') as f:\n",
        "                json.dump(self.metadata, f, indent=2)\n",
        "\n",
        "            print(f\"✅ Index saved to {filepath}\")\n",
        "\n",
        "    def load_index(self, filepath: str):\n",
        "        \"\"\"Load FAISS index from file\"\"\"\n",
        "        if os.path.exists(filepath):\n",
        "            self.index = faiss.read_index(filepath)\n",
        "\n",
        "            # Load metadata\n",
        "            metadata_path = filepath.replace('.index', '_metadata.json')\n",
        "            if os.path.exists(metadata_path):\n",
        "                with open(metadata_path, 'r') as f:\n",
        "                    self.metadata = json.load(f)\n",
        "                    self.documents = self.metadata\n",
        "\n",
        "            print(f\"✅ Index loaded from {filepath}\")\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Test the VectorDatabase\n",
        "def test_vector_database():\n",
        "    \"\"\"Test the vector database\"\"\"\n",
        "    print(\"Testing VectorDatabase...\")\n",
        "\n",
        "    # Create sample documents\n",
        "    sample_docs = [\n",
        "        {\n",
        "            'id': 'doc1_0',\n",
        "            'filename': 'accessibility.txt',\n",
        "            'content': 'Screen readers help visually impaired users navigate computers and websites.',\n",
        "            'chunk_index': 0\n",
        "        },\n",
        "        {\n",
        "            'id': 'doc1_1',\n",
        "            'filename': 'accessibility.txt',\n",
        "            'content': 'Voice assistants can control smart home devices and provide information.',\n",
        "            'chunk_index': 1\n",
        "        },\n",
        "        {\n",
        "            'id': 'doc2_0',\n",
        "            'filename': 'technology.txt',\n",
        "            'content': 'Braille displays provide tactile feedback for reading digital content.',\n",
        "            'chunk_index': 0\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Initialize and build index\n",
        "    vector_db = VectorDatabase()\n",
        "    success = vector_db.build_index(sample_docs)\n",
        "\n",
        "    if success:\n",
        "        # Test search\n",
        "        results = vector_db.search(\"How do screen readers work?\", k=2)\n",
        "        print(f\"✅ Search test completed: {len(results)} results\")\n",
        "\n",
        "    return vector_db\n",
        "\n",
        "# Run test\n",
        "if __name__ == \"__main__\":\n",
        "    test_vector_db = test_vector_database()"
      ],
      "metadata": {
        "id": "1TAfRu8I47Gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Ask for Groq API key (input is hidden by default in Colab)\n",
        "groq_api_key = input(\"Enter your Groq API key: \").strip()\n",
        "\n",
        "# Content for the .env file\n",
        "env_content = f\"\"\"GROQ_API_KEY={groq_api_key}\n",
        "\"\"\"\n",
        "\n",
        "# Path to the .env file in Colab's base directory\n",
        "env_path = '/content/.env'\n",
        "\n",
        "# Write the .env file\n",
        "with open(env_path, 'w') as f:\n",
        "    f.write(env_content)\n",
        "\n",
        "# Clear the output that might show the key\n",
        "clear_output()\n",
        "\n",
        "# Verify the file was created without showing contents\n",
        "if os.path.exists(env_path):\n",
        "    print(\"✅ .env file created successfully (contents hidden for security)\")\n",
        "else:\n",
        "    print(\"❌ Failed to create .env file\")"
      ],
      "metadata": {
        "id": "xHUwHMO3473r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First install required packages\n",
        "!pip install python-dotenv groq"
      ],
      "metadata": {
        "id": "6h0-a1Ph4-to"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# PART 6: LLM PROCESSING WITH GROQ (Llama 2 version)\n",
        "import os\n",
        "from typing import List, Dict\n",
        "from groq import Groq\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class Config:\n",
        "    GROQ_MODEL = \"llama3-8b-8192\"  # Using Llama 2 70B model\n",
        "    SUPPORTED_LANGUAGES = {\"en\": \"English\", \"es\": \"Spanish\", \"fr\": \"French\"}\n",
        "    SYSTEM_PROMPT = \"\"\"You are an AI assistant specialized in helping visually impaired users.\n",
        "    Provide clear, concise responses using simple language.\n",
        "    Always structure your answers for easy comprehension by screen readers.\"\"\"\n",
        "\n",
        "class LLMProcessor:\n",
        "    \"\"\"Handles LLM interactions using Groq API with Llama 2\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str = None):\n",
        "        self.api_key = api_key or os.getenv(\"GROQ_API_KEY\")\n",
        "        if not self.api_key:\n",
        "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY in .env or pass directly\")\n",
        "\n",
        "        self.client = Groq(api_key=self.api_key)\n",
        "        print(f\"✅ LLMProcessor initialized with {Config.GROQ_MODEL}\")\n",
        "\n",
        "    def generate_response(self, query: str, context_docs: List[Dict] = None, language: str = \"en\") -> str:\n",
        "        \"\"\"Generate response using Llama 2 via Groq\"\"\"\n",
        "        try:\n",
        "            print(f\"Processing query: '{query[:50]}...'\")\n",
        "\n",
        "            prompt = self._create_prompt(\n",
        "                query,\n",
        "                self._prepare_context(context_docs) if context_docs else \"\",\n",
        "                language\n",
        "            )\n",
        "\n",
        "            response = self.client.chat.completions.create(\n",
        "                model=Config.GROQ_MODEL,\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": Config.SYSTEM_PROMPT},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=0.5,  # Lower for more factual responses\n",
        "                max_tokens=1024,\n",
        "                top_p=0.9\n",
        "            )\n",
        "\n",
        "            result = response.choices[0].message.content.strip()\n",
        "            print(f\"✅ Generated {len(result.split())} words\")\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"LLM Error: {str(e)}\"\n",
        "            print(f\"❌ {error_msg}\")\n",
        "            return f\"Sorry, I encountered an error. Please try again later.\"\n",
        "\n",
        "    def _prepare_context(self, docs: List[Dict]) -> str:\n",
        "        return \"\\n\\n\".join(\n",
        "            f\"Document {i+1} ({d.get('filename','unnamed')}):\\n{d['content'][:400]}...\"\n",
        "            for i, d in enumerate(docs)\n",
        "        ) if docs else \"No context provided\"\n",
        "\n",
        "    def _create_prompt(self, query: str, context: str, language: str) -> str:\n",
        "        lang = Config.SUPPORTED_LANGUAGES.get(language, \"English\")\n",
        "        return f\"\"\"Respond in {lang} to this query using the context below.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Guidelines:\n",
        "1. Answer directly and factually\n",
        "2. Use simple, clear language\n",
        "3. Keep under 5 sentences unless complex\n",
        "4. Specify when information is incomplete\"\"\"\n",
        "\n",
        "def setup_environment():\n",
        "    \"\"\"Secure environment setup\"\"\"\n",
        "    load_dotenv('/content/.env')\n",
        "    if not os.getenv(\"GROQ_API_KEY\"):\n",
        "        from getpass import getpass\n",
        "        api_key = getpass(\"Enter Groq API key: \")\n",
        "        if api_key:\n",
        "            with open('/content/.env', 'w') as f:\n",
        "                f.write(f\"GROQ_API_KEY={api_key}\")\n",
        "            load_dotenv('/content/.env', override=True)\n",
        "            clear_output()\n",
        "            print(\"✅ Environment configured\")\n",
        "        else:\n",
        "            print(\"⚠️ Continuing without API key\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    setup_environment()\n",
        "\n",
        "    llm = LLMProcessor()\n",
        "\n",
        "    # Test with and without context\n",
        "    print(\"\\n--- Basic Test ---\")\n",
        "    print(llm.generate_response(\"Explain quantum computing simply\"))\n",
        "\n",
        "    print(\"\\n--- Contextual Test ---\")\n",
        "    context_docs = [{\n",
        "        'filename': 'accessibility_guide.txt',\n",
        "        'content': 'Screen readers convert text to speech and braille. Popular ones include JAWS, NVDA, and VoiceOver.'\n",
        "    }]\n",
        "    print(llm.generate_response(\n",
        "        \"What are the most common screen readers?\",\n",
        "        context_docs\n",
        "    ))"
      ],
      "metadata": {
        "id": "7GAfujdb5ESs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PART 7: MAIN RAG SYSTEM\n",
        "\n",
        "class MultilingualRAGSystem:\n",
        "    \"\"\"Main RAG system coordinating all components\"\"\"\n",
        "\n",
        "    def __init__(self, groq_api_key: str):\n",
        "        print(\"Initializing Multilingual RAG System...\")\n",
        "\n",
        "        try:\n",
        "            self.doc_processor = DocumentProcessor()\n",
        "            self.voice_processor = VoiceProcessor()\n",
        "            self.vector_db = VectorDatabase()\n",
        "            self.llm_processor = LLMProcessor(groq_api_key)\n",
        "            self.conversation_history = []\n",
        "\n",
        "            print(\"✅ RAG System initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error initializing RAG system: {e}\")\n",
        "            raise\n",
        "\n",
        "    def initialize_system(self, documents_dir: str):\n",
        "        \"\"\"Initialize the RAG system with documents\"\"\"\n",
        "        print(f\"Initializing system with documents from: {documents_dir}\")\n",
        "\n",
        "        # Load documents\n",
        "        documents = self.doc_processor.load_documents(documents_dir)\n",
        "\n",
        "        if documents:\n",
        "            # Build vector index\n",
        "            success = self.vector_db.build_index(documents)\n",
        "            if success:\n",
        "                print(\"✅ System initialization completed\")\n",
        "                return True\n",
        "\n",
        "        print(\"⚠️ System initialization completed but no documents indexed\")\n",
        "        return False\n",
        "\n",
        "    def process_voice_query_from_file(self, audio_file_path: str) -> Tuple[str, str, bytes]:\n",
        "        \"\"\"Process voice query from audio file and return text response and audio\"\"\"\n",
        "        print(f\"Processing voice query from file: {audio_file_path}\")\n",
        "\n",
        "        # Speech to text\n",
        "        query_text, detected_language = self.voice_processor.speech_to_text(audio_file_path)\n",
        "\n",
        "        if not query_text:\n",
        "            return \"\", \"Could not understand the audio input.\", b\"\"\n",
        "\n",
        "        # Process query\n",
        "        response_text = self.process_text_query(query_text, detected_language)\n",
        "\n",
        "        # Text to speech\n",
        "        response_audio = self.voice_processor.text_to_speech(response_text, detected_language)\n",
        "\n",
        "        return query_text, response_text, response_audio\n",
        "\n",
        "    def process_voice_query_from_bytes(self, audio_data: bytes) -> Tuple[str, str, bytes]:\n",
        "        \"\"\"Process voice query from audio bytes and return text response and audio\"\"\"\n",
        "        print(\"Processing voice query from audio bytes\")\n",
        "\n",
        "        # Speech to text\n",
        "        query_text, detected_language = self.voice_processor.speech_to_text_from_bytes(audio_data)\n",
        "\n",
        "        if not query_text:\n",
        "            return \"\", \"Could not understand the audio input.\", b\"\"\n",
        "\n",
        "        # Process query\n",
        "        response_text = self.process_text_query(query_text, detected_language)\n",
        "\n",
        "        # Text to speech\n",
        "        response_audio = self.voice_processor.text_to_speech(response_text, detected_language)\n",
        "\n",
        "        return query_text, response_text, response_audio\n",
        "\n",
        "    def process_text_query(self, query: str, language: str = \"en\") -> str:\n",
        "        \"\"\"Process text query and return response\"\"\"\n",
        "        print(f\"Processing text query: '{query}' (language: {language})\")\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        retrieved_docs = self.vector_db.search(query)\n",
        "\n",
        "        # Generate response\n",
        "        response = self.llm_processor.generate_response(query, retrieved_docs, language)\n",
        "\n",
        "        # Store in conversation history\n",
        "        self.conversation_history.append({\n",
        "            'timestamp': datetime.now(),\n",
        "            'query': query,\n",
        "            'language': language,\n",
        "            'response': response,\n",
        "            'retrieved_docs': len(retrieved_docs)\n",
        "        })\n",
        "\n",
        "        print(f\"✅ Query processed successfully\")\n",
        "        return response\n",
        "\n",
        "    def get_conversation_history(self) -> List[Dict]:\n",
        "        \"\"\"Get conversation history\"\"\"\n",
        "        return self.conversation_history\n",
        "\n",
        "    def clear_conversation_history(self):\n",
        "        \"\"\"Clear conversation history\"\"\"\n",
        "        self.conversation_history = []\n",
        "        print(\"✅ Conversation history cleared\")\n",
        "\n",
        "# Create sample documents for testing\n",
        "# Create sample documents for testing\n",
        "def create_sample_documents():\n",
        "    \"\"\"Create sample documents for demonstration\"\"\"\n",
        "    docs_dir = Config.DOCUMENTS_DIR\n",
        "    os.makedirs(docs_dir, exist_ok=True)\n",
        "\n",
        "    # Sample document 1: Accessibility Guide\n",
        "    with open(f\"{docs_dir}/accessibility_guide.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\"\"Accessibility Guide for Visually Impaired Users\n",
        "\n",
        "Introduction\n",
        "This guide provides essential information about accessibility tools and techniques for visually impaired individuals.\n",
        "\n",
        "Screen Readers\n",
        "Screen readers are software applications that convert text and interface elements into speech or Braille output. Popular screen readers include:\n",
        "- NVDA (NonVisual Desktop Access) - Free and open source\n",
        "- JAWS (Job Access With Speech) - Commercial screen reader\n",
        "- VoiceOver - Built into Apple devices\n",
        "- TalkBack - Built into Android devices\n",
        "\n",
        "Navigation Techniques\n",
        "Effective navigation using assistive technology involves:\n",
        "1. Learning keyboard shortcuts for faster navigation\n",
        "2. Using heading navigation to jump between sections\n",
        "3. Utilizing landmarks to understand page structure\n",
        "4. Taking advantage of skip links to bypass repetitive content\n",
        "\n",
        "Web Accessibility\n",
        "When browsing the web, look for sites that follow WCAG guidelines:\n",
        "- Proper heading structure\n",
        "- Alternative text for images\n",
        "- Keyboard-accessible controls\n",
        "- High contrast color schemes\n",
        "- Descriptive link text\"\"\")\n",
        "\n",
        "    # Sample document 2: Technology Tips\n",
        "    with open(f\"{docs_dir}/technology_tips.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\"\"Technology Tips for Enhanced Accessibility\n",
        "\n",
        "Voice Assistants\n",
        "Voice assistants can significantly improve daily productivity:\n",
        "- Set reminders and alarms\n",
        "- Control smart home devices\n",
        "- Get weather and news updates\n",
        "- Make phone calls and send messages\n",
        "- Search for information hands-free\n",
        "\n",
        "Audio Books and Podcasts\n",
        "Digital audio content provides access to vast libraries:\n",
        "- Audible and similar services offer extensive catalogs\n",
        "- Many public libraries provide free audiobook access\n",
        "- Podcast apps organize content by topics and interests\n",
        "- Speed control allows for personalized listening preferences\n",
        "\n",
        "Braille Displays\n",
        "Modern refreshable Braille displays offer:\n",
        "- Tactile feedback for digital content\n",
        "- Portable designs for mobility\n",
        "- Bluetooth connectivity with devices\n",
        "- Multiple Braille grade support\"\"\")\n",
        "\n",
        "    print(f\"✅ Created sample documents in {docs_dir}\")"
      ],
      "metadata": {
        "id": "p_pa0nUo5GyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMPLE WORKING VOICE CHATBOT FOR GOOGLE COLAB\n",
        "!pip install -q openai-whisper sentence-transformers faiss-cpu gtts pydub PyPDF2 python-docx groq python-dotenv\n",
        "\n",
        "import os, io, numpy as np, time\n",
        "from pathlib import Path\n",
        "from google.colab import files, output\n",
        "from IPython.display import Audio, display, clear_output, HTML\n",
        "import whisper\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "from gtts import gTTS\n",
        "from pydub import AudioSegment\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "from groq import Groq\n",
        "from dotenv import load_dotenv\n",
        "import base64\n",
        "\n",
        "# Configuration\n",
        "class Config:\n",
        "    GROQ_MODEL = \"llama3-8b-8192\"\n",
        "    WHISPER_MODEL = \"base\"  # Good balance between speed and accuracy\n",
        "    EMBEDDING_MODEL = \"all-MiniLM-L6-v2\"\n",
        "    DOCUMENTS_DIR = \"/content/documents\"\n",
        "    EMBEDDING_DIM = 384\n",
        "    SYSTEM_PROMPT = \"You are a helpful AI assistant. Answer concisely in 1-2 sentences.\"\n",
        "\n",
        "# Initialize\n",
        "def init():\n",
        "    # Setup environment\n",
        "    if not os.path.exists('/content/.env'):\n",
        "        print(\"Please enter your Groq API key (get it from https://console.groq.com/keys)\")\n",
        "        api_key = input(\"API Key: \").strip()\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key is required\")\n",
        "        with open('/content/.env', 'w') as f:\n",
        "            f.write(f\"GROQ_API_KEY={api_key}\\n\")\n",
        "        clear_output()\n",
        "\n",
        "    load_dotenv('/content/.env')\n",
        "    if not os.getenv('GROQ_API_KEY'):\n",
        "        raise ValueError(\"GROQ_API_KEY not found in environment variables\")\n",
        "\n",
        "    # Create documents directory\n",
        "    os.makedirs(Config.DOCUMENTS_DIR, exist_ok=True)\n",
        "\n",
        "    # Load models\n",
        "    print(\"Loading models...\")\n",
        "    models = {\n",
        "        'whisper': whisper.load_model(Config.WHISPER_MODEL),\n",
        "        'embedding': SentenceTransformer(Config.EMBEDDING_MODEL),\n",
        "        'groq': Groq(api_key=os.getenv('GROQ_API_KEY'))\n",
        "    }\n",
        "    print(\"✅ Models loaded\")\n",
        "    return models\n",
        "\n",
        "# Document Processing\n",
        "def process_uploaded_files():\n",
        "    print(\"Please upload your documents (PDF/DOCX/TXT):\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        return []\n",
        "\n",
        "    documents = []\n",
        "    for filename, content in uploaded.items():\n",
        "        filepath = os.path.join(Config.DOCUMENTS_DIR, filename)\n",
        "        with open(filepath, 'wb') as f:\n",
        "            f.write(content)\n",
        "\n",
        "        text = extract_text(filepath)\n",
        "        if text:\n",
        "            chunks = split_text(text)\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                documents.append({\n",
        "                    'filename': filename,\n",
        "                    'chunk_id': i,\n",
        "                    'content': chunk\n",
        "                })\n",
        "            print(f\"📄 {filename}: {len(chunks)} chunks extracted\")\n",
        "\n",
        "    return documents\n",
        "\n",
        "def extract_text(filepath):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        if filepath.lower().endswith('.pdf'):\n",
        "            with open(filepath, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                text = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
        "        elif filepath.lower().endswith('.docx'):\n",
        "            doc = Document(filepath)\n",
        "            text = \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "        elif filepath.lower().endswith(('.txt', '.md')):\n",
        "            with open(filepath, 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Error reading {filepath}: {str(e)}\")\n",
        "    return text\n",
        "\n",
        "def split_text(text, chunk_size=500):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "\n",
        "    for word in words:\n",
        "        if current_size + len(word) > chunk_size and current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_size = 0\n",
        "        current_chunk.append(word)\n",
        "        current_size += len(word) + 1\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def build_index(documents, embedding_model):\n",
        "    if not documents:\n",
        "        return None\n",
        "\n",
        "    print(\"Building search index...\")\n",
        "    texts = [doc['content'] for doc in documents]\n",
        "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
        "\n",
        "    index = faiss.IndexFlatIP(Config.EMBEDDING_DIM)\n",
        "    index.add(embeddings.astype('float32'))\n",
        "    print(\"✅ Index built\")\n",
        "    return index\n",
        "\n",
        "# Audio Processing\n",
        "def create_recorder_ui():\n",
        "    display(HTML(\"\"\"\n",
        "    <div style=\"text-align: center; margin: 20px; padding: 20px; border: 2px solid #4CAF50; border-radius: 10px;\">\n",
        "        <h3>🎤 Voice Chat Interface</h3>\n",
        "        <button id=\"recordBtn\" style=\"padding: 12px 24px; font-size: 16px; background: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer;\">\n",
        "            🎤 Start Recording\n",
        "        </button>\n",
        "        <div id=\"status\" style=\"margin: 15px 0; font-weight: bold;\">Ready to record</div>\n",
        "        <div id=\"timer\" style=\"font-size: 18px; color: #D32F2F;\"></div>\n",
        "    </div>\n",
        "\n",
        "    <script>\n",
        "    let mediaRecorder, audioChunks = [], isRecording = false;\n",
        "    const recordBtn = document.getElementById('recordBtn');\n",
        "    const statusDiv = document.getElementById('status');\n",
        "    const timerDiv = document.getElementById('timer');\n",
        "    let startTime, timerInterval;\n",
        "\n",
        "    recordBtn.onclick = async function() {\n",
        "        if (!isRecording) {\n",
        "            await startRecording();\n",
        "        } else {\n",
        "            stopRecording();\n",
        "        }\n",
        "    };\n",
        "\n",
        "    async function startRecording() {\n",
        "        try {\n",
        "            statusDiv.textContent = \"Starting recording...\";\n",
        "            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "            mediaRecorder = new MediaRecorder(stream);\n",
        "            audioChunks = [];\n",
        "\n",
        "            mediaRecorder.ondataavailable = e => {\n",
        "                if (e.data.size > 0) audioChunks.push(e.data);\n",
        "            };\n",
        "\n",
        "            mediaRecorder.onstop = async () => {\n",
        "                statusDiv.textContent = \"Processing...\";\n",
        "                const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });\n",
        "                const reader = new FileReader();\n",
        "                reader.onloadend = () => {\n",
        "                    const base64Audio = reader.result.split(',')[1];\n",
        "                    google.colab.kernel.invokeFunction('process_audio', [base64Audio], {});\n",
        "                };\n",
        "                reader.readAsDataURL(audioBlob);\n",
        "                stream.getTracks().forEach(track => track.stop());\n",
        "            };\n",
        "\n",
        "            mediaRecorder.start(100);\n",
        "            isRecording = true;\n",
        "            recordBtn.textContent = \"⏹️ Stop Recording\";\n",
        "            recordBtn.style.background = \"#f44336\";\n",
        "            statusDiv.textContent = \"Recording...\";\n",
        "\n",
        "            // Start timer\n",
        "            startTime = Date.now();\n",
        "            timerInterval = setInterval(updateTimer, 1000);\n",
        "            updateTimer();\n",
        "\n",
        "        } catch (err) {\n",
        "            statusDiv.textContent = \"Error: \" + err.message;\n",
        "            console.error(err);\n",
        "        }\n",
        "    }\n",
        "\n",
        "    function stopRecording() {\n",
        "        if (mediaRecorder && isRecording) {\n",
        "            mediaRecorder.stop();\n",
        "            isRecording = false;\n",
        "            clearInterval(timerInterval);\n",
        "            recordBtn.textContent = \"🎤 Start Recording\";\n",
        "            recordBtn.style.background = \"#4CAF50\";\n",
        "        }\n",
        "    }\n",
        "\n",
        "    function updateTimer() {\n",
        "        const seconds = Math.floor((Date.now() - startTime) / 1000);\n",
        "        timerDiv.textContent = `Recording: ${seconds}s`;\n",
        "    }\n",
        "    </script>\n",
        "    \"\"\"))\n",
        "\n",
        "def process_audio(base64_audio):\n",
        "    try:\n",
        "        # Save audio file\n",
        "        audio_bytes = base64.b64decode(base64_audio)\n",
        "        audio_path = \"/content/recording.webm\"\n",
        "        with open(audio_path, \"wb\") as f:\n",
        "            f.write(audio_bytes)\n",
        "\n",
        "        # Convert to WAV\n",
        "        audio = AudioSegment.from_file(audio_path)\n",
        "        audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "        wav_path = \"/content/recording.wav\"\n",
        "        audio.export(wav_path, format=\"wav\")\n",
        "\n",
        "        # Transcribe\n",
        "        result = models['whisper'].transcribe(wav_path)\n",
        "        query = result[\"text\"].strip()\n",
        "        print(f\"🎤 You asked: {query}\")\n",
        "\n",
        "        if not query or len(query) < 3:\n",
        "            return \"Sorry, I didn't hear that clearly.\"\n",
        "\n",
        "        # Get response\n",
        "        response = generate_response(query)\n",
        "        print(f\"🤖 Response: {response}\")\n",
        "\n",
        "        # Convert to speech\n",
        "        tts = gTTS(text=response, lang='en')\n",
        "        audio_buffer = io.BytesIO()\n",
        "        tts.write_to_fp(audio_buffer)\n",
        "        audio_buffer.seek(0)\n",
        "\n",
        "        # Play response\n",
        "        display(Audio(audio_buffer.read(), autoplay=True))\n",
        "        display(HTML(f\"<div style='margin:20px; padding:15px; background:#f5f5f5; border-radius:8px;'><b>Response:</b> {response}</div>\"))\n",
        "\n",
        "        # Cleanup\n",
        "        os.remove(audio_path)\n",
        "        os.remove(wav_path)\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")\n",
        "        return \"Sorry, something went wrong.\"\n",
        "\n",
        "def generate_response(query):\n",
        "    try:\n",
        "        response = models['groq'].chat.completions.create(\n",
        "            model=Config.GROQ_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": Config.SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": query}\n",
        "            ],\n",
        "            temperature=0.7,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except Exception as e:\n",
        "        return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "# Main Execution\n",
        "print(\"🔊 Voice Chatbot Initializing...\")\n",
        "models = init()\n",
        "documents = process_uploaded_files()\n",
        "index = build_index(documents, models['embedding'])\n",
        "\n",
        "# Register audio processor\n",
        "output.register_callback('process_audio', process_audio)\n",
        "\n",
        "print(\"\\n🚀 Ready to chat! Click the record button below:\")\n",
        "create_recorder_ui()\n",
        "print(\"\\n💡 Tips: Speak clearly after clicking record, and keep queries under 10 seconds.\")"
      ],
      "metadata": {
        "id": "BYDmicwl5Jd3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}